import streamlit as st
import joblib
import pandas as pd
import os
import time
from io import BytesIO
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional, Tuple
import numpy as np
import yfinance as yf
from typing import List, Dict
import requests
import pyarrow as pa
import pyarrow.parquet as pq
import io
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# ============== 1. X·ª¨ L√ù BAN ƒê·∫¶U ==============
# --- Load model ---
model_data = joblib.load("best_model.pkl")
model = model_data["model"]
numeric_cols = model_data["numeric_cols"]
label_mapping = model_data["label_mapping"]
best_model_name = model_data["best_model_name"]

# --- Page setup ---
st.markdown("<h1 style='text-align:center;text-transform: uppercase; color:#4CAF50;'> D·ª∞ ƒêO√ÅN XU H∆Ø·ªöNG TH·ªä TR∆Ø·ªúNG</h1>", unsafe_allow_html=True)
st.write(f"<p style='text-align:center; color:gray;'>Model: <b>{best_model_name}</b></p>", unsafe_allow_html=True)
st.markdown("---")

# --- CSS n·ªÅn dark blue ---
st.markdown(
    """
    <style>
        /* To√†n b·ªô n·ªÅn ·ª©ng d·ª•ng */
        .stApp {
            background-color: #0b132b; /* xanh ƒë·∫≠m */
            background-image: linear-gradient(160deg, #0b132b 0%, #1c2541 50%, #3a506b 100%);
            color: #e0e0e0;
        }

        /* Header v√† sidebar */
        [data-testid="stHeader"], [data-testid="stSidebar"] {
            background-color: #1c2541 !important;
        }

        /* M√†u ch·ªØ m·∫∑c ƒë·ªãnh */
        * {
            color: #f5f6fa;
        }

        /* Ti√™u ƒë·ªÅ */
        h1, h2, h3, h4, h5, h6 {
            color: #5bc0be !important;
        }

        /* N√∫t b·∫•m */
        .stButton button {
            background: linear-gradient(90deg, #5bc0be, #3a506b);
            color: white;
            font-weight: bold;
            border-radius: 8px;
            border: none;
            transition: 0.3s;
        }
        .stButton button:hover {
            background: linear-gradient(90deg, #3a506b, #5bc0be);
            transform: scale(1.05);
        }

        /* Input box & selectbox */
        .stTextInput, .stNumberInput, .stSelectbox {
            background-color: #1c2541;
            color: white;
        }

        /* K·∫ª b·∫£ng */
        div[data-testid="stDataFrame"] table {
            background-color: #1c2541;
            color: #f5f6fa;
        }

        /* Thanh cu·ªôn */
        ::-webkit-scrollbar {
            width: 8px;
        }
        ::-webkit-scrollbar-thumb {
            background: #5bc0be;
            border-radius: 10px;
        }
        ::-webkit-scrollbar-track {
            background: #0b132b;
        }
    </style>
    """,
    unsafe_allow_html=True
)


    # Ti√™u ƒë·ªÅ v√† menu ch·ªçn d·ªØ li·ªáu

# CONFIG
SYMBOL_MAP = {
    '^GSPC': 'sp500',
    'SPY': 'spy',
    '^VIX': 'vix',
    'GC=F': 'gold',
    'CL=F': 'oil',
    'DX-Y.NYB': 'usd_index',
    'UUP': 'uup'
}

POLYGON_API_KEY = "KXHaneBxKmIC0_oLJdUKqhRh4if7DsCz"  # thay b·∫±ng API key c·ªßa b·∫°n
MODEL_NAME = "yiyanghkust/finbert-tone"  # FinBERT
LOCAL_MODEL_DIR = "./models/finbert-tone/"
# Mapping symbol ‚Üí t√™n chu·∫©n
SYMBOL_NAME_MAP = {
    "^GSPC": "sp500",
    "SPY": "spy",
    "^VIX": "vix",
    "GC=F": "gold",
    "CL=F": "oil",
    "DX-Y.NYB": "usd_index",
    "UUP": "uup"
}

# C·∫•u h√¨nh trang
st.sidebar.markdown("### Ph·ª•c v·ª• d·ª± ƒëo√°n:")

dashboard_option = st.sidebar.selectbox(
    "Ch·ªçn ch·∫ø ƒë·ªô:",
    (
        "M√¥ h√¨nh d·ª± ƒëo√°n",
        "T·∫£i d·ªØ li·ªáu TEST"
    )
)

# Ti√™u ƒë·ªÅ ch√≠nh theo l·ª±a ch·ªçn
st.markdown(
f"<h2 style='text-align: center; text-transform: uppercase;'>{dashboard_option}</h2>",
unsafe_allow_html=True
)

# ƒê∆∞·ªùng ngƒÉn c√°ch (divider) b√™n d∆∞·ªõi menu
st.sidebar.markdown("---")

# ============== 2. H√†m h·ªó tr·ª£ x·ª≠ l√Ω ===============
# ---------------FINANCE------------------
# H√†m ƒë·ªïi t√™n c·ªôt ƒë·ªÉ hi·ªÉn th·ªã
def rename_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ƒê·ªïi t√™n c√°c c·ªôt trong DataFrame n·∫øu c·ªôt ƒë√≥ c√≥ t·ªìn t·∫°i.
    Danh s√°ch c·ªôt ƒë·ªïi t√™n ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a s·∫µn trong h√†m.
    
    Args:
        df (pd.DataFrame): DataFrame c·∫ßn ƒë·ªïi t√™n.
    
    Returns:
        pd.DataFrame: DataFrame sau khi ƒë·ªïi t√™n c·ªôt.
    """
    rename_columns = {
        "date": "Ng√†y",
        "sp500_open": "SP500 M·ªü c·ª≠a",
        "sp500_high": "SP500 Cao nh·∫•t",
        "sp500_low": "SP500 Th·∫•p nh·∫•t",
        "sp500_close": "SP500 ƒê√≥ng c·ª≠a",
        "sp500_volume": "SP500 Kh·ªëi l∆∞·ª£ng",

        "spy_open": "SPY M·ªü c·ª≠a",
        "spy_high": "SPY Cao nh·∫•t",
        "spy_low": "SPY Th·∫•p nh·∫•t",
        "spy_close": "SPY ƒê√≥ng c·ª≠a",
        "spy_volume": "SPY Kh·ªëi l∆∞·ª£ng",

        "vix_open": "VIX M·ªü c·ª≠a",
        "vix_high": "VIX Cao nh·∫•t",
        "vix_low": "VIX Th·∫•p nh·∫•t",
        "vix_close": "VIX ƒê√≥ng c·ª≠a",
        "vix_volume": "VIX Kh·ªëi l∆∞·ª£ng",

        "gold_open": "V√†ng M·ªü c·ª≠a",
        "gold_high": "V√†ng Cao nh·∫•t",
        "gold_low": "V√†ng Th·∫•p nh·∫•t",
        "gold_close": "V√†ng ƒê√≥ng c·ª≠a",
        "gold_volume": "V√†ng Kh·ªëi l∆∞·ª£ng",

        "oil_open": "D·∫ßu M·ªü c·ª≠a",
        "oil_high": "D·∫ßu Cao nh·∫•t",
        "oil_low": "D·∫ßu Th·∫•p nh·∫•t",
        "oil_close": "D·∫ßu ƒê√≥ng c·ª≠a",
        "oil_volume": "D·∫ßu Kh·ªëi l∆∞·ª£ng",

        "usd_index_open": "USD Index M·ªü c·ª≠a",
        "usd_index_high": "USD Index Cao nh·∫•t",
        "usd_index_low": "USD Index Th·∫•p nh·∫•t",
        "usd_index_close": "USD Index ƒê√≥ng c·ª≠a",
        "usd_index_volume": "USD Index Kh·ªëi l∆∞·ª£ng",

        "uup_open": "UUP M·ªü c·ª≠a",
        "uup_high": "UUP Cao nh·∫•t",
        "uup_low": "UUP Th·∫•p nh·∫•t",
        "uup_close": "UUP ƒê√≥ng c·ª≠a",
        "uup_volume": "UUP Kh·ªëi l∆∞·ª£ng",

        "sp500_return": "SP500 % Thay ƒë·ªïi",
        "sp500_range": "SP500 Bi√™n ƒë·ªô",
        "gold_return": "V√†ng % Thay ƒë·ªïi",
        "gold_range": "V√†ng Bi√™n ƒë·ªô",
        "oil_return": "D·∫ßu % Thay ƒë·ªïi",
        "oil_range": "D·∫ßu Bi√™n ƒë·ªô",

        "sp500_return_lag1": "SP500 % h√¥m tr∆∞·ªõc",
        "vix_close_lag1": "VIX h√¥m tr∆∞·ªõc",
        "gold_return_lag1": "V√†ng % h√¥m tr∆∞·ªõc",

        "market_direction": "Xu h∆∞·ªõng th·ªã tr∆∞·ªùng",
            
        # C√°c c·ªôt sentiment / tin t·ª©c
        "n_articles": "S·ªë l∆∞·ª£ng b√†i b√°o",
        "n_positive": "S·ªë b√†i t√≠ch c·ª±c",
        "n_neutral": "S·ªë b√†i trung l·∫≠p",
        "n_negative": "S·ªë b√†i ti√™u c·ª±c",

        "prop_positive": "T·ª∑ l·ªá t√≠ch c·ª±c",
        "prop_neutral": "T·ª∑ l·ªá trung l·∫≠p",
        "prop_negative": "T·ª∑ l·ªá ti√™u c·ª±c",

        "mean_sentiment_score": "ƒêi·ªÉm sentiment trung b√¨nh",
        "mean_sentiment_prob": "X√°c su·∫•t sentiment trung b√¨nh",
        "median_sentiment_prob": "X√°c su·∫•t sentiment trung v·ªã",
        "std_sentiment_score": "ƒê·ªô l·ªách chu·∫©n sentiment",
        "weighted_sentiment_score": "ƒêi·ªÉm sentiment tr·ªçng s·ªë",

        "avg_text_len": "ƒê·ªô d√†i vƒÉn b·∫£n trung b√¨nh",
        "median_text_len": "ƒê·ªô d√†i vƒÉn b·∫£n trung v·ªã"
    }

    # L·ªçc ra nh·ªØng c·ªôt th·∫≠t s·ª± c√≥ trong df
    valid_map = {old: new for old, new in rename_columns.items() if old in df.columns}
    
    # ƒê·ªïi t√™n
    return df.rename(columns=valid_map)

def normalize_columns(df):
    """
    Chu·∫©n ho√° t√™n c·ªôt t√†i ch√≠nh theo format:
    sp500_open, gold_close, oil_volume, usd_index_close, ...
    v√† gi·ªØ nguy√™n c√°c c·ªôt sentiment.
    """

    import re

    col_map = {}
    for col in df.columns:

        # 1) Gi·ªØ nguy√™n c√°c c·ªôt sentiment, s·ªë l∆∞·ª£ng b√†i, return...
        if col in [
            "date", "sp500_return", "sp500_range", "sp500_return_lag1",
            "gold_return", "gold_range", "gold_return_lag1",
            "oil_return", "oil_range", "vix_close_lag1",
            "n_articles", "n_positive", "n_neutral", "n_negative",
            "prop_positive", "prop_neutral", "prop_negative",
            "mean_sentiment_score", "mean_sentiment_prob",
            "median_sentiment_prob", "std_sentiment_score",
            "weighted_sentiment_score", "avg_text_len", "median_text_len"
        ]:
            col_map[col] = col
            continue

        # 2) Map theo ti·ªÅn t·ªë t√†i s·∫£n
        if "_^GSPC" in col: prefix = "sp500"
        elif "_SPY" in col: prefix = "spy"
        elif "_^VIX" in col: prefix = "vix"
        elif "_GC=F" in col: prefix = "gold"
        elif "_CL=F" in col: prefix = "oil"
        elif "_DX-Y.NYB" in col: prefix = "usd_index"
        elif "_UUP" in col: prefix = "uup"
        else:
            col_map[col] = col
            continue

        # 3) L·∫•y lo·∫°i gi√°: open, close, high, low, volume
        m = re.search(r"(open|close|high|low|volume)", col)
        if m:
            suffix = m.group(1)
        else:
            col_map[col] = col
            continue

        # 4) G·ªôp t√™n m·ªõi
        new_name = f"{prefix}_{suffix}"
        col_map[col] = new_name

    # 5) ƒê·ªïi t√™n to√†n DataFrame
    df = df.rename(columns=col_map)

    return df

# H√†m th√™m c√°c c·ªôt return, range, lag1
def add_financial_features(df):
    df = df.copy()

    # ----- RETURN -----
    if 'sp500_close' in df.columns:
        df['sp500_return'] = df['sp500_close'].pct_change()
    if 'gold_close' in df.columns:
        df['gold_return'] = df['gold_close'].pct_change()
    if 'oil_close' in df.columns:
        df['oil_return'] = df['oil_close'].pct_change()

    # ----- RANGE -----
    if 'sp500_high' in df.columns and 'sp500_low' in df.columns:
        df['sp500_range'] = df['sp500_high'] - df['sp500_low']
    if 'gold_high' in df.columns and 'gold_low' in df.columns:
        df['gold_range'] = df['gold_high'] - df['gold_low']
    if 'oil_high' in df.columns and 'oil_low' in df.columns:
        df['oil_range'] = df['oil_high'] - df['oil_low']

    # ----- LAG 1 -----
    if 'sp500_return' in df.columns:
        df['sp500_return_lag1'] = df['sp500_return'].shift(1)
    if 'vix_close' in df.columns:
        df['vix_close_lag1'] = df['vix_close'].shift(1)
    if 'gold_return' in df.columns:
        df['gold_return_lag1'] = df['gold_return'].shift(1)

    return df

# H√†m l·∫•y d·ªØ li·ªáu t·ª´ Yahoo Finance
def fetch_financial_data(symbol_map, start_date, end_date):
    # --- FIX: c·ªông th√™m 1 ng√†y v√†o end_date ƒë·ªÉ Yahoo Finance l·∫•y ƒë·ªß ---
    end_date_fixed = (pd.to_datetime(end_date) + pd.Timedelta(days=1)).strftime("%Y-%m-%d")

    all_data = []
    for symbol, name in symbol_map.items():
        data = yf.download(symbol, start=start_date, end=end_date_fixed, progress=False)

        if data.empty:
            st.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu cho {symbol}.")
            continue
        data = data.rename(columns={
            'Open': f'{name}_open',
            'High': f'{name}_high',
            'Low': f'{name}_low',
            'Close': f'{name}_close',
            'Adj Close': f'{name}_adj_close',
            'Volume': f'{name}_volume'
        }).reset_index().rename(columns={'Date': 'date'})
        all_data.append(data)

    if not all_data:
        return pd.DataFrame()

    df_merged = all_data[0]
    for df in all_data[1:]:
        df_merged = pd.merge(df_merged, df, on='date', how='outer')

    df_merged = df_merged.sort_values('date').reset_index(drop=True)
    return df_merged

# ---------------NEWS---------------------------
# H√†m h·ªó tr·ª£ t·∫£i b√†i b√°o an to√†n
def safe_request(url, params=None, headers=None, max_retry=4, sleep_sec=2, timeout=500):
    placeholder = st.empty()  # t·∫°o placeholder ƒë·ªÉ hi·ªÉn th·ªã tr·∫°ng th√°i
    for attempt in range(1, max_retry + 1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=timeout)
            if r.status_code in (429, 500, 502, 503):
                wait = sleep_sec * (2 ** (attempt - 1))
                placeholder.info(f"‚ö† Retry {attempt}/{max_retry} sau {wait}s do status {r.status_code}")
                time.sleep(wait)
                continue
            r.raise_for_status()
            placeholder.empty()  # x√≥a th√¥ng b√°o khi th√†nh c√¥ng
            return r.json()
        except Exception as e:
            wait = sleep_sec * (2 ** (attempt - 1))
            placeholder.info(f"‚ö† Attempt {attempt} th·∫•t b·∫°i: {e}. Retry sau {wait}s")
            time.sleep(wait)
    placeholder.empty()  # x√≥a n·∫øu h·∫øt retry
    return None

# H√†m t·∫£i b√†i b√°o t·ª´ Polygon API
def fetch_news_for_date(date_str: str, limit: int = 50) -> List[Dict]:
    """L·∫•y b√†i b√°o trong ng√†y t·ª´ Polygon"""
    url = "https://api.polygon.io/v2/reference/news"
    headers = {"Authorization": f"Bearer {POLYGON_API_KEY}"}
    params = {
        "published_utc.gte": f"{date_str}T00:00:00Z",
        "published_utc.lte": f"{date_str}T23:59:59Z",
        "limit": limit
    }
    resp = safe_request(url, params=params, headers=headers)
    articles = []
    if resp and "results" in resp:
        for a in resp["results"]:
            a["source"] = "Polygon"
            a["published_date"] = a.get("published_utc", "")[:10]
        articles = resp["results"]
        st.write(f"{date_str}: T√¨m ƒë∆∞·ª£c {len(articles)} b√†i b√°o")
    else:
        st.info(f"{date_str}: Kh√¥ng c√≥ b√†i b√°o ƒë∆∞·ª£c t√¨m th·∫•y")
    return articles

# H√†m ch·∫°y FinBERT ƒë·ªÉ ch·∫•m ƒëi·ªÉm
def load_sentiment_pipeline(model_name: str = MODEL_NAME):
    st.info(" ƒêang t·∫£i FinBERT (∆∞u ti√™n b·∫£n local)...")

    # ---- 1) Load LOCAL tr∆∞·ªõc ----
    if os.path.exists(LOCAL_MODEL_DIR):
        try:
            tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)
            model = AutoModelForSequenceClassification.from_pretrained(LOCAL_MODEL_DIR)
            nlp = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=-1)
            st.success(" FinBERT loaded t·ª´ local!")
            return nlp
        except:
            st.warning("‚ö† Kh√¥ng load ƒë∆∞·ª£c local model, chuy·ªÉn sang t·∫£i online...")

    # ---- 2) Fallback ONLINE ----
    os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    st.info(" ƒêang t·∫£i model t·ª´ HuggingFace...")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)

    # L∆∞u l·∫°i LOCAL
    tokenizer.save_pretrained(LOCAL_MODEL_DIR)
    model.save_pretrained(LOCAL_MODEL_DIR)

    nlp = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=-1)

    st.success(" T·∫£i th√†nh c√¥ng (ƒë√£ l∆∞u local ƒë·ªÉ d√πng l·∫ßn sau)")
    return nlp

# H√†m ch·∫•m ƒëi·ªÉm c·∫£m x√∫c 
def infer_sentiment(nlp, texts: List[str], batch_size: int = 16) -> List[tuple]:
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        cleaned = [t if (t and str(t).strip() != "") else "" for t in batch]
        try:
            preds = nlp(cleaned)
            for p in preds:
                label = p.get("label", "NEUTRAL").upper()
                score = float(p.get("score", 0.0))
                score_num = {"POSITIVE": 1, "NEUTRAL": 0, "NEGATIVE": -1}.get(label, 0)
                results.append((label, score, score_num))
        except Exception as e:
            st.warning(f"Inference batch failed: {e}")
            for _ in batch:
                results.append(("NEUTRAL", 0.0, 0))
    return results

def aggregate_daily_sentiment(records: List[Dict], nlp=None) -> Dict:
    """T·ªïng h·ª£p sentiment + th√™m c√°c th·ªëng k√™ n√¢ng cao."""
    if not records:
        return {
            "date": None,
            "n_articles": 0,
            "n_positive": 0,
            "n_neutral": 0,
            "n_negative": 0,
            "prop_positive": float("nan"),
            "prop_neutral": float("nan"),
            "prop_negative": float("nan"),
            "mean_sentiment_score": float("nan"),
            "mean_sentiment_prob": float("nan"),
            "median_sentiment_prob": float("nan"),
            "std_sentiment_score": float("nan"),
            "weighted_sentiment_score": float("nan"),
            "avg_text_len": float("nan"),
            "median_text_len": float("nan"),
            "articles_df": pd.DataFrame()
        }

    df = pd.json_normalize(records)

    # Chu·∫©n h√≥a text
    def prepare_text(r):
        title = r.get("title") or r.get("headline") or ""
        desc = r.get("description") or r.get("summary") or ""
        return f"{title}. {desc}".strip()

    df["text_for_sentiment"] = df.apply(lambda row: prepare_text(row.to_dict()), axis=1)
    df["text_len"] = df["text_for_sentiment"].apply(lambda x: len(str(x)))

    texts = df["text_for_sentiment"].tolist()

    if nlp is None:
        nlp = load_sentiment_pipeline()

    preds = infer_sentiment(nlp, texts)
    labels, probs, nums = zip(*preds)

    df["sentiment_label"] = labels
    df["sentiment_score_prob"] = probs
    df["sentiment_score"] = nums

    # --- Metrics ---
    n = len(df)
    n_pos = int((df["sentiment_label"] == "POSITIVE").sum())
    n_neu = int((df["sentiment_label"] == "NEUTRAL").sum())
    n_neg = int((df["sentiment_label"] == "NEGATIVE").sum())

    prop_pos = n_pos / n
    prop_neu = n_neu / n
    prop_neg = n_neg / n

    mean_score = float(df["sentiment_score"].mean())
    mean_prob = float(df["sentiment_score_prob"].mean())
    median_prob = float(df["sentiment_score_prob"].median())
    std_score = float(df["sentiment_score"].std())

    # Sentiment c√≥ tr·ªçng s·ªë theo ƒë·ªô d√†i text
    weighted_sentiment = float((df["sentiment_score"] * df["text_len"]).sum() / df["text_len"].sum())

    avg_text_len = float(df["text_len"].mean())
    median_text_len = float(df["text_len"].median())

    return {
        "date": df["published_date"].iloc[0] if "published_date" in df.columns else None,

        "n_articles": n,
        "n_positive": n_pos,
        "n_neutral": n_neu,
        "n_negative": n_neg,

        "prop_positive": prop_pos,
        "prop_neutral": prop_neu,
        "prop_negative": prop_neg,

        "mean_sentiment_score": mean_score,
        "mean_sentiment_prob": mean_prob,
        "median_sentiment_prob": median_prob,
        "std_sentiment_score": std_score,
        "weighted_sentiment_score": weighted_sentiment,

        "avg_text_len": avg_text_len,
        "median_text_len": median_text_len,

        "articles_df": df
    }





# -------------- T·ªîNG H·ª¢P -----------------
def df_to_parquet_bytes(df):
    import pyarrow as pa
    import pyarrow.parquet as pq
    table = pa.Table.from_pandas(df)
    return pq.write_table(table, where=None)



# ============== 3. TAB1 ==============
if dashboard_option == "M√¥ h√¨nh d·ª± ƒëo√°n":
    # --- Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠p li·ªáu ---
    mode1, mode2 = st.tabs(["üîπ Nh·∫≠p th·ªß c√¥ng", "üìÅ Upload file d·ªØ li·ªáu"])
    st.markdown("---")
    st.sidebar.info("N·∫øu b·∫°n mu·ªën d√πng file d·ªØ li·ªáu ƒë·ªÉ d·ª± ƒëo√°n m√† ch∆∞a c√≥ s·∫µn file, h√£y chuy·ªÉn sang tab **T·∫£i d·ªØ li·ªáu TEST**")

    # ============================================================
    # 1. Nh·∫≠p th·ªß c√¥ng
    # ============================================================
    with mode1:
        with st.form("manual_form"):
            st.markdown("### Nh·∫≠p Ch·ªâ s·ªë t√†i ch√≠nh")
            col1, col2, col3 = st.columns(3)

            with col1:
                sp500_open = st.number_input("SP500 M·ªü c·ª≠a", value=3810.0)
                sp500_high = st.number_input("SP500 Cao nh·∫•t", value=3850.0)
                sp500_low = st.number_input("SP500 Th·∫•p nh·∫•t", value=3800.0)
                sp500_close = st.number_input("SP500 ƒê√≥ng c·ª≠a", value=3840.0)
                sp500_volume = st.number_input("SP500 Kh·ªëi l∆∞·ª£ng", value=3900000000.0)

                sp500_return = st.number_input("SP500 % Thay ƒë·ªïi", value=0.0)
                sp500_range = st.number_input("SP500 Bi√™n ƒë·ªô", value=37.0)
                sp500_return_lag1 = st.number_input("SP500 % h√¥m tr∆∞·ªõc", value=0.0)

                spy_open = st.number_input("SPY M·ªü c·ª≠a", value=370.0)
                spy_high = st.number_input("SPY Cao nh·∫•t", value=375.0)
                spy_low = st.number_input("SPY Th·∫•p nh·∫•t", value=365.0)
                spy_close = st.number_input("SPY ƒê√≥ng c·ª≠a", value=372.0)
                spy_volume = st.number_input("SPY Kh·ªëi l∆∞·ª£ng", value=85900000.0)

                oil_return = st.number_input("D·∫ßu % Thay ƒë·ªïi", value=0.0)
                oil_range = st.number_input("D·∫ßu Bi√™n ƒë·ªô", value=4.6)


            with col2:
                vix_open = st.number_input("VIX M·ªü c·ª≠a", value=22.0)
                vix_high = st.number_input("VIX Cao nh·∫•t", value=23.0)
                vix_low = st.number_input("VIX Th·∫•p nh·∫•t", value=21.9)
                vix_close = st.number_input("VIX ƒê√≥ng c·ª≠a", value=22.5)
                vix_volume = st.number_input("VIX Kh·ªëi l∆∞·ª£ng", value=0.0)

                vix_close_lag1 = st.number_input("VIX h√¥m tr∆∞·ªõc", value=22.5)

                gold_open = st.number_input("V√†ng M·ªü c·ª≠a", value=1850.0)
                gold_high = st.number_input("V√†ng Cao nh·∫•t", value=1880.0)
                gold_low = st.number_input("V√†ng Th·∫•p nh·∫•t", value=1849.0)
                gold_close = st.number_input("V√†ng ƒê√≥ng c·ª≠a", value=1872.0)
                gold_volume = st.number_input("V√†ng Kh·ªëi l∆∞·ª£ng", value=62.0)

                gold_return = st.number_input("V√†ng % Thay ƒë·ªïi", value=0.0)
                gold_range = st.number_input("V√†ng Bi√™n ƒë·ªô", value=13.0)
                gold_return_lag1 = st.number_input("V√†ng % h√¥m tr∆∞·ªõc", value=0.0)



            with col3:
                oil_open = st.number_input("D·∫ßu M·ªü c·ª≠a", value=77.2)
                oil_high = st.number_input("D·∫ßu Cao nh·∫•t", value=77.4)
                oil_low = st.number_input("D·∫ßu Th·∫•p nh·∫•t", value=72.8)
                oil_close = st.number_input("D·∫ßu ƒê√≥ng c·ª≠a", value=73.5)
                oil_volume = st.number_input("D·∫ßu Kh·ªëi l∆∞·ª£ng", value=350000.0)

                usd_index_open = st.number_input("USD Index M·ªü c·ª≠a", value=103.0)
                usd_index_high = st.number_input("USD Index Cao nh·∫•t", value=104.0)
                usd_index_low = st.number_input("USD Index Th·∫•p nh·∫•t", value=102.5)
                usd_index_close = st.number_input("USD Index ƒê√≥ng c·ª≠a", value=103.5)
                usd_index_volume = st.number_input("USD Index Kh·ªëi l∆∞·ª£ng", value=0.0)

                uup_open = st.number_input("UUP M·ªü c·ª≠a", value=25.2)
                uup_high = st.number_input("UUP Cao nh·∫•t", value=25.3)
                uup_low = st.number_input("UUP Th·∫•p nh·∫•t", value=25.1)
                uup_close = st.number_input("UUP ƒê√≥ng c·ª≠a", value=25.28)
                uup_volume = st.number_input("UUP Kh·ªëi l∆∞·ª£ng", value=4400000.0)


            st.markdown("---")
            st.markdown("### Nh·∫≠p Th√¥ng tin tin t·ª©c v√† Ch·ªâ s·ªë c·∫£m x√∫c")

            col4, col5, col6 = st.columns(3)
            with col4:
                n_articles = st.number_input("S·ªë l∆∞·ª£ng b√†i b√°o", value=50.0)
                n_positive = st.number_input("S·ªë b√†i t√≠ch c·ª±c", value=36.0)
                n_neutral = st.number_input("S·ªë b√†i trung l·∫≠p", value=8.0)
                n_negative = st.number_input("S·ªë b√†i ti√™u c·ª±c", value=6.0)
                prop_positive = st.number_input("T·ª∑ l·ªá t√≠ch c·ª±c", value=0.72)

            with col5:
                prop_neutral = st.number_input("T·ª∑ l·ªá trung l·∫≠p", value=0.16)
                prop_negative = st.number_input("T·ª∑ l·ªá ti√™u c·ª±c", value=0.12)
                mean_sentiment_score = st.number_input("ƒêi·ªÉm sentiment trung b√¨nh", value=0.56)
                weighted_sentiment_score = st.number_input("ƒêi·ªÉm sentiment tr·ªçng s·ªë", value=0.9657)
                mean_sentiment_prob = st.number_input("X√°c su·∫•t sentiment trung b√¨nh", value=0.9)

            with col6:
                median_sentiment_prob = st.number_input("X√°c su·∫•t sentiment trung v·ªã", value=0.75)
                std_sentiment_score = st.number_input("ƒê·ªô l·ªách chu·∫©n sentiment", value=0.57)
                avg_text_len = st.number_input("ƒê·ªô d√†i vƒÉn b·∫£n trung b√¨nh", value=225.0)
                median_text_len = st.number_input("ƒê·ªô d√†i vƒÉn b·∫£n trung v·ªã", value=186.0)


            submit_manual = st.form_submit_button(" B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n")

        if submit_manual:
            # gom d·ªØ li·ªáu th√†nh DataFrame
            input_data = pd.DataFrame([[locals()[col] for col in numeric_cols]], columns=numeric_cols)
            pred = model.predict(input_data)[0]
            prob = model.predict_proba(input_data)[0]
            inv_map = {v: k for k, v in label_mapping.items()}
            pred_label = inv_map[pred]

            st.success(" D·ª± ƒëo√°n ho√†n t·∫•t!")

            color = "#2E8B57" if pred_label == "up" else "#C0392B"
            st.markdown(f"<h3 style='text-align:center; color:{color};'> D·ª± ƒëo√°n Xu h∆∞·ªõng th·ªã tr∆∞·ªùng: {pred_label.upper()}</h3>", unsafe_allow_html=True)
            st.progress(prob[pred])
            st.write(f"**M·ª©c ƒë·ªô ch√≠nh x√°c:** {prob[pred]*100:.2f}%")

    # ============================================================
    # 2. Upload file d·ªØ li·ªáu
    # ============================================================
    with mode2:
        st.markdown("###  Upload File D·ªØ Li·ªáu ƒë·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n")
        st.markdown("""
            Ph·∫ßn n√†y c·∫ßn d√πng file d·ªØ li·ªáu c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p ƒë·ªÉ d·ª± ƒëo√°n. 
            N·∫øu ch∆∞a c√≥ s·∫µn file d·ªØ li·ªáu, h√£y Ch·ªçn ch·∫ø ƒë·ªô *T·∫£i d·ªØ li·ªáu TEST* trong `Sidebar` ƒë·ªÉ t·∫£i d·ªØ li·ªáu.

            ---
            """)

        uploaded_file = st.file_uploader("T·∫£i file d·ªØ li·ªáu", type=["parquet"])

        if uploaded_file is not None:
            df1 = pd.read_parquet(uploaded_file)
            st.dataframe(df1)
            st.success(" File ƒë√£ ƒë·ªçc th√†nh c√¥ng!")
            df_new = df1.dropna()

            # Ki·ªÉm tra c·ªôt h·ª£p l·ªá
            missing_cols = [c for c in numeric_cols if c not in df_new.columns]
            if missing_cols:
                st.error(f"‚ö†Ô∏è File thi·∫øu c√°c c·ªôt c·∫ßn thi·∫øt: {missing_cols}")
            else:
                st.markdown("""
                                
                ƒê·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n c·∫ßn b·ªè qua nh·ªØng h√†ng d·ªØ li·ªáu b·ªã **NaN**, d∆∞·ªõi ƒë√¢y l√† d·ªØ li·ªáu h·ª£p l·ªá sau x·ª≠ l√Ω t·ª´ d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i l√™n ƒë·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n.

                ---
                """)

                st.dataframe(df_new)

                st.markdown("---")

                st.markdown("<h4 style='color:#4CAF50;'> B·∫•m n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n:</h4>", unsafe_allow_html=True)
                
                # N√∫t k√≠ch ho·∫°t d·ª± ƒëo√°n
                predict_button = st.button(" B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n")

                if predict_button:
                    preds = model.predict(df_new[numeric_cols])
                    probs = model.predict_proba(df_new[numeric_cols])
                    inv_map = {v: k for k, v in label_mapping.items()}

                    # Sau khi d·ª± ƒëo√°n
                    df_new["Prediction"] = [inv_map[p] for p in preds]
                    df_new["Confidence (%)"] = (probs.max(axis=1) * 100).round(2)

                    st.success(" D·ª± ƒëo√°n ho√†n t·∫•t!")
                    st.markdown("###  K·∫øt qu·∫£ d·ª± ƒëo√°n:")

                    st.markdown("""
            - C·ªôt *Prediction* hi·ªÉn th·ªã **D·ª± ƒëo√°n Xu h∆∞·ªõng th·ªã tr∆∞·ªùng** ng√†y ti·∫øp theo. 
            - C·ªôt *Confidence (%)* hi·ªán th·ªã **M·ª©c ƒë·ªô tin c·∫≠y c·ªßa d·ª± ƒëo√°n**.
            - B·∫°n c√≥ th·ªÉ ki·ªÉm ch·ª©ng k·∫øt qu·∫£ d·ª± ƒëo√°n b·∫±ng c√°ch xem *sp500_close* c·ªßa ng√†y h√¥m sau.
    
            """)

                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ an to√†n
                
                    cols_to_add = df_new.columns.difference(["Prediction", "Confidence (%)"])
                    df_to_show = df_new[["Prediction", "Confidence (%)"]].join(df_new[cols_to_add])

                    # ƒê∆∞a sp500_close l√™n v·ªã tr√≠ th·ª© 3
                    cols = list(df_to_show.columns)

                    if "sp500_close" in cols:
                        cols.remove("sp500_close")
                        cols.insert(2, "sp500_close")  # v·ªã tr√≠ th·ª© 3 (index 2)
                        df_to_show = df_to_show[cols]

                    # H√†m highlight c·ªôt
                    def highlight_col(col):
                        styles = []
                        for _ in col:
                            if col.name == "sp500_close":
                                styles.append('background-color: yellow; font-weight: bold')
                            elif col.name == "Prediction":
                                styles.append('background-color: blue; font-weight: bold')
                            elif col.name == "Confidence (%)":
                                styles.append('background-color: green; font-weight: bold')
                            else:
                                styles.append('')
                        return styles
                    
                    # Hi·ªÉn th·ªã DataFrame tr√™n Streamlit v·ªõi highlight
                    st.dataframe(df_to_show.style.apply(highlight_col))

                    # N√∫t t·∫£i v·ªÅ
                    csv = df_new.to_csv(index=False).encode("utf-8")
                    st.download_button(
                        label=" T·∫£i k·∫øt qu·∫£ (CSV)",
                        data=csv,
                        file_name="predictions.csv",
                        mime="text/csv",
                    )

if dashboard_option == "T·∫£i d·ªØ li·ªáu TEST":
    # --- Ch·ªçn ch·∫ø ƒë·ªô nh·∫≠p li·ªáu ---
    mode1, mode2, mode3 = st.tabs(["üîπ D·ªØ li·ªáu t√†i ch√≠nh", "üîπ D·ªØ li·ªáu tin t·ª©c", "üìÅ T·ªïng h·ª£p d·ªØ li·ªáu"])
    st.markdown("---")

    # --- Sidebar c·∫•u h√¨nh ---
    start_date = st.sidebar.date_input("Ch·ªçn ng√†y b·∫Øt ƒë·∫ßu", value=date(2025, 11, 1))
    end_date = st.sidebar.date_input("Ch·ªçn ng√†y k·∫øt th√∫c", value=date.today() - timedelta(days=1))

    if start_date > end_date:
        st.error("Ng√†y b·∫Øt ƒë·∫ßu ph·∫£i <= ng√†y k·∫øt th√∫c")
        st.stop()

    # --- Kh·ªüi t·∫°o version v√† reset khi ƒë·ªïi ng√†y ---
    if "last_start" not in st.session_state:
        st.session_state.last_start = start_date
    if "last_end" not in st.session_state:
        st.session_state.last_end = end_date
    if "data_version" not in st.session_state:
        st.session_state.data_version = 0
    if "last_data_version" not in st.session_state:
        st.session_state.last_data_version = 0

    if start_date != st.session_state.last_start or end_date != st.session_state.last_end:
        # Reset Mode1
        st.session_state.data_loaded = False
        st.session_state.features_generated = False
        st.session_state.df_raw = pd.DataFrame()

        # Reset Mode2
        st.session_state.step1_done = False
        st.session_state.step2_done = False
        st.session_state.news_data = {}
        st.session_state.sentiment_data = {}
        st.session_state.df_sentiment_summary = pd.DataFrame()

        # TƒÉng version ƒë·ªÉ Mode2 nh·∫≠n bi·∫øt d·ªØ li·ªáu m·ªõi
        st.session_state.data_version += 1
        st.session_state.last_data_version = st.session_state.data_version

        # C·∫≠p nh·∫≠t l·∫°i ng√†y
        st.session_state.last_start = start_date
        st.session_state.last_end = end_date


    # =================TAB1==================
    with mode1:

        st.markdown("###  B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu t√†i ch√≠nh t·ª´ Yahoo Finance")
        
        st.markdown("""
            ƒê·ªÉ b·∫Øt ƒë·∫ßu, h√£y ch·ªçn kho·∫£ng th·ªùi gian b·∫°n mu·ªën t·∫£i d·ªØ li·ªáu t·∫°i `Sidebar` v√† b·∫•m n√∫t d∆∞·ªõi ƒë√¢y.
            """)

        # --- B∆∞·ªõc 1: N√∫t t·∫£i d·ªØ li·ªáu ---
        if st.button("T·∫£i d·ªØ li·ªáu t√†i ch√≠nh"):
            # --- TƒÉng version m·ªói khi d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c t·∫£i ---
            st.session_state.data_version = st.session_state.get("data_version", 0) + 1
            with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
                df_raw = fetch_financial_data(SYMBOL_MAP, start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"))
                df_raw.columns = ['_'.join(col).strip() for col in df_raw.columns.values]



            if df_raw.empty:
                st.error("Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng th·ªùi gian n√†y.")
                st.session_state.data_loaded = False
            else:
                st.session_state.df_raw = df_raw
                st.session_state.data_loaded = True
                st.session_state.features_generated = False
                st.success(f"ƒê√£ t·∫£i {len(df_raw)} ng√†y d·ªØ li·ªáu.")

        # --- Hi·ªÉn th·ªã b·∫£ng 1 n·∫øu c√≥ d·ªØ li·ªáu ---
        if st.session_state.get("data_loaded", False):
            st.markdown(
                "<h3 style='text-align: center;'>B·∫£ng 1: D·ªØ li·ªáu g·ªëc</h3>",
                unsafe_allow_html=True
            )

            st.dataframe(st.session_state.df_raw, use_container_width=True)

            # --- N√∫t ti·∫øp t·ª•c x·ª≠ l√Ω ---
            if st.button("Ti·∫øp t·ª•c x·ª≠ l√Ω"):
                with st.spinner("ƒêang x·ª≠ l√Ω d·ªØ li·ªáu..."):
                    # 1) T·∫°o c√°c c·ªôt features c∆° b·∫£n

                    df_features_1 = st.session_state.df_raw.copy()
                    df_features_1 = normalize_columns(df_features_1)
                
                    df_features = add_financial_features(df_features_1)

                    #df_features_extended = pd.DataFrame(list(df_features))

                # L∆∞u v√†o session
                st.session_state.df_features = df_features
                st.session_state.features_generated = True

        # --- Hi·ªÉn th·ªã b·∫£ng 2 n·∫øu ƒë√£ x·ª≠ l√Ω ---
        if st.session_state.get("features_generated", False):
            st.markdown(
                "<h3 style='text-align: center;'>B·∫£ng 2 : D·ªØ li·ªáu sau x·ª≠ l√Ω</h3>",
                unsafe_allow_html=True
            )
            st.dataframe(st.session_state.df_features, use_container_width=True)
            st.success(
                "B·∫°n ƒë√£ ho√†n th√†nh t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu t√†i ch√≠nh, h√£y k√©o l√™n ƒë·∫ßu trang v√† chuy·ªÉn sang tab **D·ªØ li·ªáu tin t·ª©c** ƒë·ªÉ l√†m b∆∞·ªõc ti·∫øp theo"
            )


    with mode2:
        
        st.markdown("###  B∆∞·ªõc 2: T·∫£i d·ªØ li·ªáu tin t·ª©c v√† ch·∫•m ƒëi·ªÉm c·∫£m x√∫c")

        if "initialized_news" not in st.session_state:
            st.session_state.initialized_news = True
            st.session_state.last_data_version = st.session_state.get("data_version", 0)

        # N·∫øu d·ªØ li·ªáu t√†i ch√≠nh thay ƒë·ªïi ‚Üí reset TAB2
        if st.session_state.get("last_data_version") != st.session_state.get("data_version"):
            st.session_state.step1_done = False
            st.session_state.step2_done = False
            st.session_state.news_data = {}
            st.session_state.sentiment_data = {}

            # c·∫≠p nh·∫≠t version ƒë√£ x·ª≠ l√Ω
            st.session_state.last_data_version = st.session_state.get("data_version")


        # --- Ki·ªÉm tra b∆∞·ªõc 1 ---
        if not st.session_state.get("data_loaded", False) or not st.session_state.get("features_generated", False):
            st.warning("‚ö† B·∫°n c·∫ßn ho√†n t·∫•t B∆∞·ªõc 1 (t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu t√†i ch√≠nh) tr∆∞·ªõc khi ƒë·∫øn ƒë√¢y.")
        else:
            # N·ªôi dung ch√≠nh b∆∞·ªõc 2 ch·ªâ hi·ªán khi b∆∞·ªõc 1 ƒë√£ xong
            st.markdown("""
                ƒê·ªÉ ti·∫øp t·ª•c, ch√∫ng t√¥i x√°c nh·∫≠n ng√†y b·∫°n ch·ªçn t·ª´ "B∆∞·ªõc 1".
            """)


            # --- Hi·ªÉn th·ªã x√°c nh·∫≠n ng√†y ---
            st.info(f"**Kho·∫£ng th·ªùi gian ƒë√£ ch·ªçn:** t·ª´ **{start_date.strftime('%d/%m/%Y')}** ƒë·∫øn **{end_date.strftime('%d/%m/%Y')}**")

            # Ch·ªçn kho·∫£ng ng√†y
            limit_articles = st.number_input("Ch·ªçn s·ªë b√†i t·ªëi ƒëa m·ªói ng√†y", min_value=1, max_value=100, value=50, step=1)

            # --- T·∫°o c√°c v√πng giao di·ªán c·ªë ƒë·ªãnh ---
            box_step1 = st.container()
            box_step2 = st.container()
            box_step3 = st.container()

            # --- Step 1: T·∫£i d·ªØ li·ªáu ---
            with box_step1:
                if "step1_done" not in st.session_state:
                    st.session_state.step1_done = False
                if "news_data" not in st.session_state:
                    st.session_state.news_data = {}

                if not st.session_state.step1_done:
                    if st.button(" T·∫£i d·ªØ li·ªáu tin t·ª©c"):
                        st.session_state.news_data = {}
                        cur_date = start_date
                        total_days = (end_date - start_date).days + 1
                        pbar = st.progress(0)
                        day_idx = 0

                        while cur_date <= end_date:
                            date_str = cur_date.strftime("%Y-%m-%d")
                            articles = fetch_news_for_date(date_str, limit=limit_articles)
                            st.session_state.news_data[date_str] = articles or []

                            day_idx += 1
                            pbar.progress(day_idx / total_days)
                            cur_date += timedelta(days=1)

                        st.session_state.step1_done = True
                        st.success(" ƒê√£ t·∫£i xong d·ªØ li·ªáu.")

                # Hi·ªÉn th·ªã d·ªØ li·ªáu n·∫øu ƒë√£ t·∫£i xong
                if st.session_state.step1_done:
                    with st.expander(" Xem d·ªØ li·ªáu tin t·ª©c t·ª´ng ng√†y"):
                        for date_str, articles in st.session_state.news_data.items():
                            st.markdown(f"**{date_str}** - {len(articles)} b√†i")
                            if articles:
                                st.dataframe(
                                    pd.DataFrame(articles)[["published_date", "title", "description"]]
                                )

            # --- Step 2: Ch·∫•m sentiment ---
            with box_step2:
                st.markdown("---")

                if st.session_state.step1_done and not st.session_state.step2_done:
                    if st.button(" Ch·∫•m ƒëi·ªÉm Sentiment"):
                        nlp_model = load_sentiment_pipeline()
                        st.session_state.sentiment_data = {}

                        for date_str, articles in st.session_state.news_data.items():
                            if articles:
                                daily_sentiment = aggregate_daily_sentiment(articles, nlp=nlp_model)
                                st.session_state.sentiment_data[date_str] = daily_sentiment

                                with st.expander(f" Sentiment ng√†y {date_str}"):
                                    st.write(f"S·ªë b√†i: {daily_sentiment['n_articles']}")
                                    st.write(pd.DataFrame([{
                                        k: v for k, v in daily_sentiment.items() if k != "articles_df"
                                    }]))

                        st.session_state.step2_done = True
                        st.success(" Ho√†n t·∫•t ch·∫•m ƒëi·ªÉm sentiment t·ª´ng ng√†y.")

                elif st.session_state.step2_done:

                    # --- Hi·ªÉn th·ªã l·∫°i to√†n b·ªô expander v√† b·∫£ng d·ªØ li·ªáu ---
                    for date_str, daily_sentiment in st.session_state.sentiment_data.items():
                        with st.expander(f" Sentiment ng√†y {date_str}"):
                            st.write(f"S·ªë b√†i: {daily_sentiment['n_articles']}")
                            st.write(pd.DataFrame([{
                                k: v for k, v in daily_sentiment.items() if k != "articles_df"
                            }]))


            # --- Step 3: T·ªïng h·ª£p sentiment ---

                with box_step3:
                    st.markdown("---")
                    if st.session_state.step2_done:
                        if st.button(" T·ªïng h·ª£p sentiment theo ng√†y"):
                            st.session_state.df_sentiment_summary = pd.DataFrame([
                                {k: v for k, v in s.items() if k != "articles_df"}
                                for s in st.session_state.sentiment_data.values()
                            ])
                            st.subheader(" Summary t·ªïng h·ª£p t·∫•t c·∫£ ng√†y")
                            st.dataframe(st.session_state.df_sentiment_summary)
                            if "df_sentiment_summary" not in st.session_state:
                                st.session_state.df_sentiment_summary = pd.DataFrame()

                            st.success(
                "B·∫°n ƒë√£ ho√†n th√†nh t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu tin t·ª©c, h√£y k√©o l√™n ƒë·∫ßu trang v√† chuy·ªÉn sang tab **T·ªïng h·ª£p d·ªØ li·ªáu** ƒë·ªÉ l√†m b∆∞·ªõc ti·∫øp theo"
            )



    with mode3:
        st.markdown("###  B∆∞·ªõc 3: T·ªïng h·ª£p d·ªØ li·ªáu t√†i ch√≠nh v√† tin t·ª©c ƒë√£ x·ª≠ l√Ω")
        # --- Ki·ªÉm tra B∆∞·ªõc 2 + version ---
        if st.session_state.get("last_data_version", 0) != st.session_state.get("data_version", 0):
            st.warning("‚ö† B·∫°n c·∫ßn ho√†n t·∫•t B∆∞·ªõc 2 v·ªõi d·ªØ li·ªáu t√†i ch√≠nh m·ªõi tr∆∞·ªõc khi ƒë·∫øn ƒë√¢y.")
        elif not st.session_state.get("step2_done", False) or st.session_state.get("df_sentiment_summary") is None:
            st.warning("‚ö† B·∫°n c·∫ßn ho√†n t·∫•t B∆∞·ªõc 2 (tin t·ª©c + ch·∫•m sentiment + t·ªïng h·ª£p) tr∆∞·ªõc khi ƒë·∫øn ƒë√¢y.")
        else:

            if st.session_state.get("step2_done", False):
                st.markdown("""
            Sau khi b·∫°n ƒë√£ t·∫£i d·ªØ li·ªáu t√†i ch√≠nh v√† tin t·ª©c v√† x·ª≠ l√Ω th√†nh c√¥ng, h√£y b·∫•m n√∫t d∆∞·ªõi ƒë√¢y ƒë·ªÉ t·ªïng h·ª£p l·∫°i th√†nh m·ªôt file s·∫µn s√†ng th·ª±c hi·ªán d·ª± ƒëo√°n

            ---
            """)
                if st.button(" Ch·∫°y t·ªïng h·ª£p d·ªØ li·ªáu th√†nh b·∫£n Final s·∫µn s√†ng d·ª± ƒëo√°n"):


                    # L·∫•y d·ªØ li·ªáu t·ª´ session_state
                    df_all = st.session_state.df_sentiment_summary
                    df_features = st.session_state.df_features.copy()

                    df_all['date'] = pd.to_datetime(df_all['date'], format="%Y-%m-%d")
                    df_features['date_'] = pd.to_datetime(df_features['date_'], format="%Y-%m-%d")
                    df_features = df_features.rename(columns={'date_': 'date'})

                    df_merged = pd.merge(df_features, df_all, on='date', how='inner')
                    st.dataframe(df_merged)


                    # Chuy·ªÉn df th√†nh file parquet trong RAM
                    buffer = io.BytesIO()
                    table = pa.Table.from_pandas(df_merged)
                    pq.write_table(table, buffer)
                    buffer.seek(0)

                    # T·∫°o t√™n file t·ª´ ng√†y b·∫Øt ƒë·∫ßu / k·∫øt th√∫c
                    file_name = f"summary_{start_date.strftime('%Y-%m-%d')}_to_{end_date.strftime('%Y-%m-%d')}.parquet"

                    # N√∫t t·∫£i file parquet
                    st.download_button(
                        label=" T·∫£i v·ªÅ file Parquet",
                        data=buffer,
                        file_name=file_name,
                        mime="application/octet-stream"
                    )



